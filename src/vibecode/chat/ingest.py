"""
PDF Ingestion Engine for VibeChat.
Parses LLM PDFs generated by LLMRenderer to extract file contents and metadata.
Prioritizes Digital Twin Manifest for 100% fidelity.
"""

import re
import json
import zlib
import base64
import logging
from typing import Dict, Optional
from dataclasses import dataclass
from pypdf import PdfReader

logger = logging.getLogger(__name__)


@dataclass
class PDFContext:
    """Represents the extracted context from a Vibecode LLM PDF."""
    files: Dict[str, str]  # Mapping of file_path -> file_content
    total_tokens: int  # Estimated token count
    tree: str  # Directory tree structure
    total_chars: int  # Total character count
    

def parse_pdf(pdf_path: str) -> PDFContext:
    """
    Parse a Vibecode LLM PDF and extract file contents.
    
    Strategy (Priority Order):
    1. Digital Twin Manifest (VIBECODE_RESTORE_BLOCK) - 100% fidelity
    2. Legacy text scraping (START_FILE/END_FILE markers) - fallback
    
    Args:
        pdf_path: Path to the LLM PDF file
        
    Returns:
        PDFContext with extracted files, token count, and tree
        
    Raises:
        FileNotFoundError: If PDF doesn't exist
        ValueError: If PDF format is invalid
    """
    logger.info(f"Loading PDF context from: {pdf_path}")
    
    try:
        reader = PdfReader(pdf_path)
    except FileNotFoundError:
        logger.error(f"PDF not found: {pdf_path}")
        raise FileNotFoundError(f"PDF not found: {pdf_path}")
    except Exception as e:
        logger.error(f"Failed to read PDF: {e}")
        raise ValueError(f"Failed to read PDF: {e}")
    
    # Extract all text from PDF
    full_text = ""
    try:
        for page in reader.pages:
            full_text += page.extract_text() + "\n"
    except Exception as e:
        logger.error(f"Failed to extract text from PDF: {e}")
        raise ValueError(f"Failed to extract text from PDF: {e}")
    
    if not full_text.strip():
        raise ValueError("PDF contains no extractable text")
    
    # Extract directory tree (used by both strategies)
    tree = _extract_tree(full_text)
    
    # PRIORITY 1: Try Digital Twin Manifest (from llm.py lines 162-175)
    files = _extract_manifest(full_text)
    
    if files:
        logger.info(f"✓ Digital Twin Manifest found. Extracted {len(files)} files with 100% fidelity.")
    else:
        # PRIORITY 2: Fallback to legacy text scraping
        logger.warning("⚠ Manifest not found. Falling back to legacy text scraping.")
        files = _extract_files(full_text)
        logger.info(f"Extracted {len(files)} files using legacy scraping.")
    
    if not files:
        raise ValueError("No files found in PDF. Ensure it was generated by LLMRenderer.")
    
    # Calculate stats
    total_chars = sum(len(content) for content in files.values())
    total_tokens = total_chars // 4  # Same heuristic as LLMRenderer
    
    logger.info(f"Context loaded: {len(files)} files, ~{total_tokens:,} tokens")
    
    return PDFContext(
        files=files,
        total_tokens=total_tokens,
        tree=tree or "(No tree found)",
        total_chars=total_chars
    )




def _extract_manifest(text: str) -> Optional[Dict[str, str]]:
    """
    Extract files from Digital Twin Manifest (VIBECODE_RESTORE_BLOCK).
    This provides 100% fidelity by using the compressed JSON manifest
    created by LLMRenderer (llm.py lines 162-175).
    
    Args:
        text: Full PDF text content
        
    Returns:
        Dictionary of {file_path: content} if manifest found, None otherwise
    """
    import hashlib
    
    # Pattern matches the manifest block from llm.py
    manifest_pattern = r"--- VIBECODE_RESTORE_BLOCK_START ---\s*(.*?)\s*--- VIBECODE_RESTORE_BLOCK_END ---"
    match = re.search(manifest_pattern, text, re.DOTALL)
    
    if not match:
        return None
    
    try:
        raw_payload = match.group(1).strip()
        
        # Handle checksum format (Extension 1): sha256:<hash>\n<payload>
        if raw_payload.startswith("sha256:"):
            try:
                checksum_line, payload = raw_payload.split('\n', 1)
                expected_hash = checksum_line.split(':')[1].strip()
                
                # Verify integrity (SCRUB all whitespace/newlines caused by PDF wrapping)
                clean_payload = "".join(payload.split())
                actual_hash = hashlib.sha256(clean_payload.encode('utf-8')).hexdigest()
                
                if actual_hash != expected_hash:
                    logger.warning(f"Checksum mismatch! Expected {expected_hash[:8]}..., got {actual_hash[:8]}...")
                    # Try to restore anyway
                else:
                    logger.debug("Checksum verified.")
                    
                payload = clean_payload
            except ValueError:
                logger.warning("Malformed checksum header. Attempting legacy decode...")
                payload = raw_payload
        else:
            # Legacy format (no checksum)
            payload = raw_payload
        
        # Decode base64
        compressed_data = base64.b64decode(payload)
        
        # Decompress zlib
        json_bytes = zlib.decompress(compressed_data)
        
        # Parse JSON
        files = json.loads(json_bytes.decode('utf-8'))
        
        return files
        
    except Exception as e:
        logger.warning(f"Failed to parse manifest: {e}. Falling back to legacy scraping.")
        return None


def _extract_tree(text: str) -> Optional[str]:
    """Extract the project tree from CONTEXT: PROJECT STRUCTURE section."""
    # Look for the tree section
    tree_pattern = r"CONTEXT: PROJECT STRUCTURE\s*\n.*?Below is the file tree.*?\n\n(.*?)(?=\n\n---|\Z)"
    match = re.search(tree_pattern, text, re.DOTALL)
    
    if match:
        tree = match.group(1).strip()
        return tree
    
    return None


def _extract_files(text: str) -> Dict[str, str]:
    """
    Extract file contents using START_FILE/END_FILE markers.
    
    Pattern from LLMRenderer:
    --- START_FILE: path/to/file.py ---
    <file content>
    --- END_FILE ---
    """
    files = {}
    
    # Regex to capture file path and content
    pattern = r"--- START_FILE:\s*(.+?)\s*---\s*\n(.*?)\n--- END_FILE ---"
    
    matches = re.finditer(pattern, text, re.DOTALL)
    
    for match in matches:
        file_path = match.group(1).strip()
        content = match.group(2)
        
        # Remove any leading/trailing whitespace from content
        # but preserve internal structure
        content = content.rstrip()
        
        files[file_path] = content
    
    return files


def get_file_content(context: PDFContext, file_path: str) -> Optional[str]:
    """
    Retrieve specific file content from parsed context.
    
    Args:
        context: Parsed PDFContext
        file_path: Relative path to the file (as appears in PDF)
        
    Returns:
        File content if found, None otherwise
    """
    return context.files.get(file_path)


def get_context_summary(context: PDFContext) -> str:
    """
    Generate a human-readable summary of the PDF context.
    
    Args:
        context: Parsed PDFContext
        
    Returns:
        Summary string with file count, token estimate, etc.
    """
    file_count = len(context.files)
    token_estimate = context.total_tokens
    
    # Determine context tier
    if token_estimate < 32000:
        tier = "Small project - fits in all LLMs"
    elif token_estimate < 128000:
        tier = "Medium project - fits in GPT-4/Claude/Gemini"
    else:
        tier = "Large project - may need RAG fallback"
    
    return (
        f"Loaded {file_count:,} files\n"
        f"Estimated tokens: {token_estimate:,}\n"
        f"Context tier: {tier}"
    )
